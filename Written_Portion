James Ambat
CS-462
Assignment 5

                                               Written Portion
Part 1.
Our Mars rover has been out collecting samples, and it needs to return to the charging station as quickly as possible.

It knows that over rocky terrain it can go 2 km/h. Over sandy terrain it can go 3 km/h, and over smooth terrain it can
go 5 km/h.

There are three routes it might choose from. Unfortunately, our terrain data for the three routes is incomplete, so we
only have estimates.

    Route 1 is 2 km long. There is a 20% chance it is sandy, 30% chance it is smooth, and a 50% chance it is rocky.
    Route 2 is 1.8 km long. There is a 40% chance it is sandy, a 20% chance it is smooth, and a 40 % chance it is rocky.
    Route 3 is 3.1 km long. There is a 50% chance it is sandy, a 40% chance it is smooth, and a 10% chance it is rocky.

**(10 points)** Which route should we pick? Show your work.
Sandy:  3 km/h
Smooth: 5 km/h
Rocky:  2 km/h

Route 1
-----------------------------------------------------------------
  Expected Speed Route 1 = 0.2 * 3 + 0.3 * 5 + 0.5 * 2
                         = 3.1 km/h

     Travel Time Route 1 = Distance / Speed
                         = 2 km / 3.1 km/h
                         = 0.6451 hrs

Route 2
-----------------------------------------------------------------
  Expected Speed Route 2 = 0.4 * 3 + 0.2 * 5 + 0.4 * 2
                         = 3.0 km/h

     Travel Time Route 2 = Distance / Speed
                         = 1.8 km / 3.0 km/h
                         = 0.6 hrs  *** LEAST TRAVEL TIME

Route 3
-----------------------------------------------------------------
  Expected Speed Route 3 = 0.5 * 3 + 0.4 * 5 + 0.1 * 2
                         = 3.7 km/h

     Travel Time Route 3 = Distance / Speed
                         = 3.1 km / 3.7 km/h
                         = 0.8378 hrs
RESPONSE:
----------------------------------------------------------------
* We should pick Route 2 because it has the least travel time.


We have now found out some additional information.
    Route 1 contains a crater. If the wall of the crater is intact, we can go through it.
    If the wall has been damaged, we will need to go around, which will add 45 minutes to our journey.
    There is a 30% chance that the wall is damaged.

    Route 2 contains a bridge. If that bridge is damaged, we will need to repair it, which
    will add 1 hour to our time. There is a 60% chance that the bridge is out.

**(10 points)** Now which route should we pick? Show your work.
If Route 1's crater wall is intact, it will take 0.6451 hrs.
If Route 1's crater wall is damaged, it will take 0.75 hrs + 0.6451 hrs = 1.3951 hrs.
* The expected utility Route 1 = 0.7 * 0.6451 hrs + 0.3 * 1.3951 hrs
                               = 0.8701 hrs
                               (pay up to 0.22499 hrs for this info: 0.8701 - 0.6451 = 0.22499)


If Route 2's bridge is not damaged, it will take 0.6 hrs.
If Route 2's bridge is damaged, it will take 1.6 hrs (1 + 0.6 = 1.6).
* The expected utility Route 2 = 0.4 * 0.6 hrs + 0.6 * 1.6 hrs
                               = 1.2 hrs
                               (pay up to 0.6 hrs for this info: 1.2 - 0.6 = 0.6)

* The expected utility Route 3 = 0.8378 hrs, which is the same from the previous problem


Route 1: if we pay up to 0.22499 hours to know if the crater wall is intact, it will take 0.8701 hrs (0.22499 + 0.6451).
Route 1: if we pay up to 0.22499 hours and find the crater wall is not intact, it will take 1.620 hrs (0.22499 + 1.3951).


Route 2: if we pay up to 0.6 hours to know if the bridge is not damaged, it will take a total of: 1.2 hrs (0.6 + 0.6)
Route 2: if we pay up to 0.6 hours and find the bridge is damaged, it will take a total of: 2.2 hours (1.6 + 0.6)

Route 3: the expected time is 0.8378 hrs ***NOT SMOOTH *** ()

RESPONSE:
------------------------
* We should pick route 3


**(10 points)** Now suppose that we can use a satellite to find out whether the terrain in route 3 is smooth.
Is this helpful? What is the value of this information? Expressed differently, how long are we
willing to wait for this information from the satellite?

Route 3
-----------------------------------------------------------------
Sandy:  3 km/h
Smooth: 5 km/h
Rocky:  2 km/h

 ORIGINAL EXPECTED SPEED = 0.5 * 3 + 0.4 * 5 + 0.1 * 2
                         = 3.7 km/h

     Travel Time Route 3 = Distance / Speed
                         = 3.1 km / 3.7 km/h
                         = 0.8378 hrs

  Expected Speed Route 3 SMOOTH = 5 km/h
     Travel Time Route 3 SMOOTH = Distance / Speed
                         = 3.1 km / 5 km/h
                         = 0.62 hrs
                         (pay up to 0.2178 hrs for this info: 0.8378 - .62 = 0.2178)

Route 3: if we pay 0.2178 hrs to know if surface is smooth, it will take a total of:
     0.8378 hrs (0.62 + 0.2178)

If not SMOOTH: 3.1 km / 2 km/h = 1.55 hrs ONLY IF we stay on Route 3
    * So we can choose from route 1 or route 2:
        * 0.8701 hrs from route 1
        * 1.2    hrs from route 2

RESPONSE:
---------------------------------------------------------------------
Choose route 3 if smooth
Choose route 1 is not smooth

**(5 points)** Now put this problem into ChatGPT. Is it able to solve it correctly? If not, where does it make mistakes?
CHAT GPT is **not** able to solve it.

* For Question 1.1, it incorrectly calculates the travel times using the speeds.
* For Question 1.2, it incorrectly uses the speeds vs. the travel time to calculate the perfect values information.
* For Question 1.3, it again incorrectly uses speeds vs. travel time and does not use the perfect value information
from Question 1.2 to make the decision.



Question 4
[AINow](https://ainowinstitute.org/) is a research institute that produces policy analysis addressing the concentration
of power in the tech industry.
They have recently published a landscape report assessing the state of the AI industry and making policy recommendations.

Please read the [executive summary](https://ainowinstitute.org/wp-content/uploads/2023/04/Exec-Summary-AI-Now-2023-Landscape-Report-.pdf) and answer the following questions:

- What are the three dimensions along which Big Tech has an advantage in AI?
The three dimensions are the data advantage, computing power, and geopolitical advantage.
    * Data advantage: From the reading, I take this to mean that Big Tech companies that have access to raw data can
    surveil massive data banks and control the market of AI products just by the fact that they can access and process
    large amounts of data and create products and services from it. A smaller company cannot really compete against Big
    Tech that has the infrastructure and access to all the data needed to create AI products.

    * Computing Power. From the reading, I take this to mean that Big Tech has the resources (both infrastructure, and
    labor pool) to actually create products that compete at scale. And while other AI companies can make products,
    they are still using Big Tech's infrastructures, tools, and even data to create their products.

    * Geopolitical. From the reading, I take this to mean that since Big Tech's AI products have become interrlated to
    the USA's and other countries dominance over the world, the focus and attention on Big Tech (including their
    time, money, attention, and interests) are at the forefront of political, military, and societal discussion and
    policy making. I find this a little disturbing since there are many other societal and economic problems that our
    world and human society can solve to make the world a safer and more peaceful planet vs. all the hype and attention
    focussed on AI products.

- Why does AI Now think it's important to focus on Big Tech?
AI Now thinks it's important to focus on Big Tech since the focus can:
    * (1) help communities, governments, and societies identify and address critical concerns.
        * Some of the concerns are data surveillance, patterns of inequality and discrimiation, and consolidation of
        power.
    * (2) temper, address, and understand knock-on effects of Big Tech's influence in certain markets.
        * Big Tech has been shown to influence market trends that end up shaping business models and sometime in
        negative, ineffective, or counter productive means to society.
    * (3) prevent Big Tech from becoming infrastructural and a single point of failure due to an over reliance on
    Big Tech.
        * With an over-reliance on Big Tech, humans and governments are not only making Big Tech a single point of
        failure, we are really handing over all the resources, time, energy, and attention to a business model that we
        can argue is not even productive to solving some of our world's toughest problems.

- Priority 1 discusses Algorithmic Accountability. What does this mean? Why is it important to shift responsibility
for detecting harm on companies themselves?
This means that the Tech companies themselves must be accountable for the harms and impacts their products cause vs.
third party regulators or investigative personnel uncovering damaging effects of the AI products that are being
manufactured and consumed. AI Now highlights that leaning on investigative work with audits may further entrench power
and systemic issues within Big Tech firms and distract from actually addressing violations or lack of accountability for
harmful practices that are creating chaotic business models and harming societies. Additionally, the accountability is
placed on the those agencies that have very limited technical, financial, and auditing resources. With such a large
landscape to audit it may be nearly impossible for a small organized group of auditors, with limited technical resources,
to conduct complete and fair audits within a reasonable timeline.
AI Now also highlights that the systems which need to be audited are very large; so large that with the given lack of
audit requirements, and identifiable harms, would make it difficult for auditors to even come up with a template for
which to audit the products for risks and harms. Given the aforementioned limitations, external audits devolve into
"check in the box" exercises. Because of this it is ever more important for Big Tech to provide
reasonably available means to access the data so that the public and vetted researchers can evaluate the harms and
hold Big Tech accountable for their products. Lastly by making Big Tech demonstrate that their products are fit for
public use and public consumption (through reporting, analysis, impact statements, and publicly available data)
more clarity on risks, standards, and even the lack of assessments becomes Big tech's burden to carry and not the
resource-limited auditors.


- What are the windows for action that are identified? Which do you personally think are the most effective or promising?
The windows for action are:
    (1) Contain tech firm's data advantage.
        * This means limiting the a companies data advantage by utilizing clear rules in data collection, connecting
        privacy laws and competition laws in development of AI policies, scrutinizing consolidation of data advantages
        through mergers.
    (2) Build support for competition reforms to reduce concentration in tech.
        * This means aggressively curbing mergers and acting strong penalties for anti-competitive actions.
        * This also means recognizing the affects of any "AI race" hype that can affect policy and regulations.
        * Anti-trust enforcers should be given the strongest tools to challenge abusive practices in Big Tech.
        * Competition analysis should be applied across all tech policy domains.
    (3) Regulate ChatGPT and other large-scale models
        * This ensures that more regulation and scrutiny should be applied to AI products when they are used for
        specific purposes and tasks.
        * This also requires heavy documentation so that developers can be held accountable for their products.
        * Laws should be formally published so that AI product owners can be publicly accountable.
        * This also means heavily scrutinizing the openness of their data resources.

    (4) Displace audits as primary policy response to harmful AI.
        * Another way to hold Big Tech accountable is needed. Audits further entrench Big Tech's power.
        * Strong bans, rules, and limitations are needed to curb irresponsible rollouts of AI products.
    (5) Future proof against quiet expansion of biometric surveillance into new domains.
        * Regulation needs to be published with very clear limitations to biometric surveillance so that
        biometric surveillance cannot transpose itself to other domains without repercussion or visibility.
        * Strong data minimization efforts and policies are needed to further impeded unnecessary advancement of
        harmful biometric surveillance.
    (6) Enact strong curbs on worker surveillance.
        * Workers should have more control of their data and biometrics than the employers.
        * To implement this, clear guidelines and laws are needed ensure workers are protected.
        * Developers and Employers should be held accountable for managing the data, not simply having the workers
        accept and waive liability.
        * Regulation of automated employee functions are needed.
    (7) Prevent international preemption by digital trade agreements that can be used to weaken national regulation on
    algorithmic accountability and competition policy.
        * This means that Big Tech cannot use trade agreements to exploit loop holes for regualtion that would have
        other wise applied in other countries.
        * This also means preventing the disclosure of code or algorithms just because a trade agreement is in place.

The windows I think will be the most effective are:
(1) Contain tech firm's data advantage,
(3) Regulate ChatGPT and other large-scale models,
(4) Displace audits as primary policy response to harmful AI, and
(5) Future proof against quiet expansion of biometric surveillance into new domains.

I think that acting on these 4 windows now will be the most effective to proactively (vs. reactively) deal with the
responsible role out of AI products. I agree with AI Now that an audit based approach cannot scale with the speed and
growth that Big Tech moves. This lack of velocity has caused auditors and even regulation rollouts to miss out on
recognizing that Big Tech actually has a monopoly over all our data, is re-feeding humans back this safe-guarded data
to make more money, all while real-human problems like world peace, hunger, energy consumption, and homelessness
ARE NOT being solved with new these technologies. Moreover, when a startup company tries to make an AI product,
the Big Tech somehow claims rights and power over the data and technologies used to "invent" AI products for human
consumption. Containing tech firm's data advantage will help the responsible build out of AI product from a different
lense; a lense that is not driven by profits, moving fast, and monopolizing over creativity and innovations.
Additionally regulating ChatGPT with very strong accountability measures is needed to prevent abuse and lack of
accountability from developers when harmful risks become actual hazards. The developers and organizations cannot
simply just walk away and claim that it's just the "unintended side effects of the software". If these harmful side
effects ensue, the developers and companies of these new LLMs need to be accountable and liable, and just be able
to walk away from the problem just because there are no regulations -- the quicker we can enact regulations, the
less entrenched and "lawless" this new landscape of LLMs can become. And if AI companies are not subjected to mere
audits but should be held liable for their reporting, impact statements, studies, and consumption research,
AI companies will think more than twice about just shipping a product for the hype, profits, and visibility; they will
be forced to think about (and thoroughly study) the repercussions and harms of their products. And by
future-proofing biometric collection with blatantly and strong regulation, we can prevent derivatives of harms that
come from aggressive surveillance, while containing the damage that may have ensure from a real AI mishap.

- The executive summary contains this quote:

"These are only a handful of examples, and what they make clear is that there is nothing about
artificial intelligence that is inevitable. Only once we stop seeing AI as synonymous with progress
can we establish popular control over the trajectory of these technologies and meaningfully confront
their serious social, economic, and political impacts—from exacerbating patterns of inequality in
housing, credit, healthcare, and education to inhibiting workers’ ability to organize and incentivizing
content production that is deleterious to young people’s mental and physical health."

Do you agree with this assessment? How might we rethink our relationship with AI and with technology in order to
avoid these potential negative outcomes?
I do agree with this assessment. Current trends, news, hype, and even just where the money is all flowing, is
affecting decision making of people, organizations, governments, militaries, and even Big Tech companies invest
and operate ways that don't always help people and societies; but just because all the momentum and money is flowing
in a certain direction does not mean humans, societies, and people just have to accept trends, harmful affects, or
deeply entrenched Big Tech companies and their products. We should rethink our relationship with AI as if we are
directly consuming it like food, drink, and medicines. People should be empowered, educated, and knowledgeable
enough to understand the AI products that are being used on them (whether directly or indirectly) just as if a person
is able to read the nutrition label on any regulated food, beverage, or medicine. Additionally, food, beverages, and
medicines are highly regulated with lots of testing, research, and human trials before they are "released into the
wild". And even when they are released the regulation and oversight allows for the responsible drawback and recall
of hazardous effect and products. The same approach needs to be applied with AI. An example I think of applies to
self driving cars. I find it hard to believe that a self-driving car can be released into the wild without having
to take a "DMV driver's license test" with the same scrutiny and rigor as any person undergoes. But somehow, the
money, profits, and Big Tech influence allows it to happen. If we rethink our relationship with AI be as highly
regulated as food, beverages, and medicines, it would vastly change the way we as humans create human-centered AI
products.


Now paste this last question into ChatGPT and include its response. How do you compare its output to your own?

The ChatGPT response appears to hone in on keywords and attempts to guess at the context of these key words and
provide a human-like response. However, the explanation of the key words and it's attempt to explicitly answer the
question is a little out of context. The LLM even says that it has no opinions but only provides some analysis as
outlined in it's review of they key words it identifies.




